\section*{4.3 Fibonacci探索}
図4.3に示すように不確実な範囲$I_k = \lbrack x_{L,k},
x_{U,k} \rbrack$ 及び $I_k$に位置する$x_{a,k},
x_{b,k}$ について考える。 4.2節のように、$f(x_{a,k}),
f(x_{b,k})$で書かれる、$x_{a,k},
x_{b,k}$における$f(x)$の値は、 もし,$f(x_{a,k}) \textless{}
f(x_b,k)$であれば、左側の範囲を $I_{k+1}^{L} =
\lbrack x_{L,k}, x_{b,k}
\rbrack\rbrack$として選択するため、

もし,$f(x_{a,k}) \textgreater{} f(x_b,k)$であれば、右側の範囲を
$I_{k+1}^{R} = \lbrack x_{a,k}, x_{U,k}
\rbrack$ として選択するため、

さもなくば、$f(x_a, k) = f(x_b,
k)$であれば、$I_{k+1}^R$と$I_{k+1}^L$
のどちらかを選択するために使われる。

もし、右の区間$I^R_{k+1}$が選択された場合、その範囲は最小点を含み、
それに加え、$f(x)$の値は$I^R_{k+1}$の内点の一点でわかっている、すなわち$x_{b,k}$の点である。
また$f(x)$が$I^R_{k+1}$内点の一つ以上の点、つまり$x_{b,
k+1}$の点で評価されるとき、
さらに不確実な区間を縮小するための十分な情報が得られ、そして、上記の処理サイクルは繰り返される。
図4.3で示されるような$I_{k+2}^L$と$I_{k+2}^R$の二つの新しい部分区間のうち一つを、
前述のように選択することが可能であり、これが繰り返される。
この方法では、一回の繰り返しに対し関数評価が一つだけしか要求されず、
計算量は二分探索法で要求される計算量と比較して削減されるだろう。

図4.3では

$$I_k = I_{k+1}^L + I_{k+2}^R
\ \text{(4.1)}$$

としていて、そして便宜上、同一の区間である場合、

$$I_{k+1}^L = I_{k+1}^R = I_{k+1}$$
$$I_{k+2}^L = I_{k+2}^R = I_{k+2}$$

式$4.1$は

$$I_k = I_{k+1} + I_{k+2} \ \text{(4.2)}$$

という漸化式を与えている。

上記の手順が何回も繰り返されるとき、区間列 $\lbrace
I_1, I_2, \cdots , I_n \rbrace$
は次のようにして得られる

$$ \begin{array}{c} I_1 = I_2 + I_3
\\ I_2 = I_3 + I_4
\\ \vdots
\\ I_n = I_{n+1} + I_{n+2}
\\ \end{array} $$

上記の一連の$n$個の方程式では、$n +
2$個の変数があり、そして$I_1$が初期区間として与えられた時 $n +
1$個の変数が残る。
それゆえに、無限数列は何らかの追加のルールを特定することで作れる。
特に興味深い2つの特殊な順序はFibonacci数列と黄金分割数列である。
Fibonacci数列はこの章で、黄金分割数列は4.4章で考える。

Fibonacci数列は$n+2$回目の繰り返しで零になる範囲、つまり$I_{n+2} =
0$を仮定することで得られる。 式$4.2$で$k =
n$と置けば、次のように書ける。 $$ \begin{array}{c}
I_{n+1} = I_n - I_{n+2} = I_n \equiv F_0I_n
\\ I_{n} = I_{n+1} + I_{n+2} =
I_n \equiv F_1I_n \\
I_{n-1} = I_n + I_{n+1} = 2I_n \equiv F_2I_n
\\ I_{n-2} = I_{n-1} + I_{n} =
3I_n \equiv F_3I_n \\
I_{n-3} = I_{n-2} + I_{n-1} = 5I_n \equiv
F_4I_n \\ I_{n-4} = I_{n-3} +
I_{n-2} = 8I_n \equiv F_5I_n
\\ \vdots
\\ I_k = I_{k+1} + I_{k+2} = F_{n
- k + 1}I_n \ \cdots\
(4.3a) \\ \vdots
\\ I_1 = I_2 + I_3 = F_nI_n
\ \cdots\ (4.3b)
\end{array} $$

得られた数列、すなわち $\lbrace1, 1, 2, 3, 5, 8, 13,
\cdots \rbrace = \lbrace
F_0, F_1, F_2, F_3, F_4, F_5, F_6, \cdots
\rbrace $
の数列は、数学の様々な分野に存在するFibonacci数列として有名である。

それは$F_0 = F_1 = 1 $ の下での再帰的な関係によって生まれ、 $$ F_k
= F_{k-1} + F_{k-2} \ \text{for} \ k
\geq 2 \ \text{(4.4)} $$ である。

一次元最適化におけるこれの応用はFibonacci探索法である。 この方法の、$n
= 6, I_1 = 100 $で始まり、常に左側の範囲が選択された場合、
つまり最小値が$x = 0$の近傍で存在する場合が、図4.4で描かれている。

もし、繰り返しの回数が$n$になると仮定すると、式$(4.3b)$から、Fibonacci探索は不確実な範囲を
$$I_n = \frac{I_1}{F_n}
\ \text{(4.5)}$$ に縮小する。

例えば、もし$n = 11$なら、$F_n = 144$
なので、$I_n$は$I_1$の値の$1\%$未満の値に縮小される。
これは11回の反復を伴い、各反復に一回の関数の評価が必要になるので、
同じだけの正確さを得るために14回の関数評価が必要な二分探索法に対して、合計11回の関数の評価が必要になる。
事実上Fibonacci探索は二分探索に比べて効率が良い。
それにまた、他の探索法に比べて最も大きな範囲の削減を達成していることも示されている、
そしてしたがって、必要計算量の面においても最も効率的である。

区間のFibonacci数列は$n$が明らかになっている場合のみ得られる。
もし、最適化の目的が規定された許容誤差の範囲で$x^{\ast}$を見つけることの場合、
要求される$n$は式$(4.5)$を用いることで用意に推測できる。
しかしながら、もし目的が$f(x)$の最小値を規定された許容誤差の範囲で見つけることの場合、
問題を解かずに要求される$n$を決めるのは難しくなるだろう。
唯一得られる情報は、もし$f(x)$の最小値が浅く、$f(x)$が解の近傍で急速に変化する場合に、
$n$が小さくなるだろうということである。

上の原理はFibonacci探索を実装するために使われる。初期の最小化値の境界を仮定しよう、
いわゆる$x_{L,1}$と$x_{U,1}$である、そして$n$の値が与えられ、$f(x)$の数学的表現が存在する。
この実装には連続した範囲を計算すること、$f(x)$を評価すること、そして適切な範囲を選択することからなっている。

$k$回目の繰り返しでは、$x_{L,k}, x_{a,k}, x_{b,k},
x_{U,k}, I_{k+1}$の量と

$f_{a,k} = f(x_{a,k}), f_{b,k} = f(x_{b,k})$

が分かる、そして、 $x_{L,k+1}, x_{a,k+1}, x_{b,k+1},
x_{U,k+1}, I_{k+2}, f_{a,k+1}, f_{b,k+1}$の量が必要となる。
$I_{k+2}$の範囲は

$$I_{k+2} =
\frac{F_{n-k-1}}{F_{n-k}}I_{k+1}
\ \text{(4.6)}$$

のようにして式$(4.3a)$から得られる。

残った量は以下の様にして計算される。

$f_{a,k} \textgreater{}
f_{b,K}$ならば、$x^{\ast}$は$\lbrack
x_{a,k} , x_{U,k} \rbrack$ の範囲の中にあり、
従って$x^{\ast}$の境界は $$x_{L,k+1} =
x_{a,k} \ \text{(4.7)}$$ $$x_{b,k+1} = x_{L,k+1} +
I_{k+2} \ \text{(4.8)}$$ の様に更新される。

似たように、新しい範囲の二つの内点、いわゆる$x_{a,k+1}$と$x_{b,k+1}$は
それぞれ$x_{b,k}$と$x_{L,k+1} + I_{k+2}$になるだろう。
このように図4.5に示されるように $$x_{a, k+1} = x_{a,k}
\ \text{(4.9)}$$ $$x_{b, k+1} = x_{L,k+1} + I_{k+2}
\ \text{(4.10)}$$ の様に割り当てられる。

$f_{b,k}$の値は$x_{a,k+1}$における$f(x)$の値として保持され、$x_{b,k+1}$での$f(x)$の値は計算される、つまり
$f_{a,k+1} = f_{b,k} \ \text{(4.11)}$ $f_{b,k+1}
= f(x_{b,k+1}) \ \text{(4.12)}$ である。

一方で、$f_{a,k} \textless{}
f_{b,k}$の場合、$x^{\ast}$は$\lbrack
x_{L,k}, x_{b,k} \rbrack$の範囲の中にある。
この場合では、図4.6で示されるように $$x_{L,k+1} = x_{L,k}
\ \text{(4.13)}$$ $$x_{U,k+1} = x_{b,k}
\ \text{(4.14)}$$ $$x_{a,k+1} = x_{U,k+1} - I_{k+2}
\ \text{(4.15)}$$ $$x_{b,k+1} = x_{a,k}
\ \text{(4.16)}$$ $$f_{b,k+1} = f_{a,k}
\ \text{(4.17)}$$ のように割り当て、 $$f_{a,k+1} =
f(x_{a,k+1}) \ \text{(4.18)}$$ のように計算する。

運悪く$f_{a,k} = f_{b,k}$の場合、
$x^{\ast}$が$\lbrack x_{L,k} ,
x_{b,k} \rbrack$と$\lbrack x_{a,k}
, x_{U,k} \rbrack$ のどちらの範囲にも含まれるので
上のどちらかの割り当てが使われる。

上記の手順は図4.7に示されるように、 $$ I_{k+2} = I_n$$
の場合、$k=n-2$になるまで繰り返され、 $$
x^\ast = x_{a,k+1} = x_{b, k+1} $$
明らかに,最小値はある許容誤差$\pm 1 / F_n$の間の値に決まるだろう。

$x^\ast$の誤差は、二分探索法の1段階を適用することで二分の一になるだろう。
これは、$\epsilon \textless{} 1 / F_n$の下で、

$$ x^\ast = \begin{cases}
x_{a,k+1} + \frac{1}{2F_n} & \text{if} \ f(x_{a,k+1} + \epsilon ) \textless{} f(x_{a,k+1}) \\
x_{a,k+1} + \frac{\epsilon}{2} & \text{if} \ f(x_{a,k+1} + \epsilon ) = f(x_{a,k+1}) \\
x_{a,k+1} - \frac{1}{2F_n} & \text{if} \ f(x_{a,k+1} + \epsilon ) \textgreater{} f(x_{a,k+1}) \\
\end{cases} $$

と$x^\ast$を定めたときに、点$x=x_{a,k+a} +
\epsilon$で$f(x)$を評価することで達成される。

もし$n$がとても大きい場合、$x_{a,k}$と$x_{b,k}$の差はとても小さくなる、
そして$x_{a,k}$は丸め誤差のために、$x_{b.k}$を超える可能性がある。
もしこれが起きたとき、信頼できない結果が得られることになる。
そのような適用の場合、
もし丸め誤差が発生すれば、問題を取り除くために、アルゴリズムの中に誤差の検査が組み込まれることになる。
おそらく、$x_{a,k} \approx
x_{b,K}$であれば、十分な精度が達成されるため、
一つの可能性($x_{a,k} \approx
x_{b,K}$の場合)がアルゴリズムを終了させるでしょう。

上記の原理は次のアルゴリズムによって構築される。

\newpage

\subsection*{アルゴリズム4.1 Fibonacci探索}\label{-4-1-fibonacci-}

\subsubsection*{Step1}\label{step1}

$x_{L,1},x_{U,1}$と$n$を入力する

\subsubsection*{Step2}\label{step2}

式($4,4$)を用いて、$F_1, F_2, \dots ,
F_n$を計算する

\subsubsection*{Step3}\label{step3}

$I_1 = x_{U,1} -x_{L,1}$を割り当て、 \\
$I_2 = \frac{F_{n-1}}{F_n} I_1$(式$(4.6)$を見よ) \\
$
x_{a,1} = x_{U,1} - I_2, \ x_{b,1} = x_{L,1} + I_2 \\
f_{a,1} = f(x_{a,1}), \ f_{b,1} = f(x_{b,1}).
$ \\
を計算し、$k = 1$と置く

\subsubsection*{Step4}\label{step4}

$I_{k+2}$を式$(4.6)$を用いて計算する。 \\
もし $f_{a.k} \geq
f_{b,k}$であるとき、$x_{L,k+1},x_{U,k+1},x_{b,k+1},f_{a,k+1},f_{b,k+1}$を
式$(4.7)$から$(4.12)$を使って更新する。 \\
さもなくば、もし$f_{a,k} \textless{}
f_{b.k}$であれば式$(4.13)$から$(4.18)$を使って情報を更新する。

\subsubsection*{Step5}\label{step5}

もし$k = n - 2$または$x_{a,k+1} \textgreater{}
x_{b,k+1}$であれば、 $x^\ast =
x_{a,k+1}$と$f^\ast =
f(x^\ast)$を出力し、終了する。 \\
さもなくば、$k = k + 1$と置き、Step4からの手順を繰り返す。 \\
$x_{a,k+1} \textgreater{} x_{b,k+1}$の条件は、
前述の通り、利用したコンピュータの精度で \\
$x_{a,k+1} \approx x_{b,k+1}$であること、または、
このアルゴリズムに何らかの誤りがあることを暗示している。
このように、この条件は代替終了基準として用いられる。
